# üß∞ My Working Stack

## üñ•Ô∏è Programming & Query Languages

These are the core languages I use to build, automate, and model solutions ‚Äî from semantic layers to scripting to interface design.

- **Python** ‚Äî scripting, automation, data analysis, scraping, and integration
- **SQL** ‚Äî querying, transformation, and modeling across BI and backend systems
- **DAX** ‚Äî advanced measure creation and expression logic for Power BI models
- **Power Query (M)** ‚Äî data transformation and shaping inside Power BI
- **TMDL** ‚Äî semantic modeling and deployment for enterprise-scale tabular models
- **Lua** ‚Äî daily use for customizing and extending my Neovim configuration (via LazyVim)
- **C#** ‚Äî scripting in Tabular Editor for DAX automation and metadata handling
- **JavaScript** ‚Äî used for scripting in workflow automation and custom widgets
- **HTML/CSS** ‚Äî used for styling and customizing visual elements in web-based reports and dashboards

## üìä Data Tools & BI Platforms

These are the primary platforms and tools I use to design, deploy, and maintain enterprise-ready BI and reporting solutions.

- **Power BI** ‚Äî my core platform for report development, modeling, and insight delivery

  - **Power BI Desktop** ‚Äî for model design, data transformation (Power Query), and report layout
  - **Power BI Service** ‚Äî for dataset publishing, dataflows, pipelines, row-level security, and workspace management

- **Microsoft Fabric** ‚Äî early adopter of Fabric as a unified analytics platform

  - Working with **Lakehouses**, **Warehouses**, **Pipelines**, and **Semantic Models** (via TMDL)

- **Tabular Editor** ‚Äî used for advanced semantic modeling, automation, and deployment

  - Authoring and editing TMDL or Tabular Object Model (TOM)
  - Scripting with C# for model manipulation and optimization
  - Managing calculation groups, translations, metadata, and source control

- **DAX Studio** ‚Äî essential for performance tuning, diagnostics, and query inspection

  - Evaluating DAX query plans and storage engine behavior
  - Measuring performance with Server Timings and Query Plan
  - Testing and refining DAX expressions in isolation

- **Excel** ‚Äî often used for rapid validation, ad hoc data profiling, and business-oriented modeling
  - Verifying model output via pivot tables connected to datasets
  - Creating sanity checks and reconciliation reports
  - Supporting hybrid workflows with non-technical users

## ü§ñ AI, LLMs & Automation

I actively integrate AI tooling into my workflows ‚Äî both to accelerate development and to create intelligent, adaptable systems for clients and internal use.

- **Prompt Engineering** ‚Äî designing modular, context-aware prompts for analysis, content creation, automation, and structured logic chains

- **LLM Tooling** ‚Äî working across multiple platforms for prototyping, insight generation, and workflow integration:

  - **OpenAI** ‚Äî for structured prompt systems, content extraction, and assistant-style interfaces
  - **Microsoft Copilot** ‚Äî for productivity acceleration across documentation and reporting tools
  - **Ollama** ‚Äî experimenting with lightweight, private LLMs for offline and local-enhanced tasks

- **Generative AI** ‚Äî using LLMs for interface sketching, documentation scaffolding, semantic tagging, and rapid knowledge transfer

- **AI-Augmented Workflows** ‚Äî supporting tasks like data profiling, column descriptions, metric definitions, text-to-query translation, and QA/testing scenarios

- **RAG and Vector Search (Exploratory)** ‚Äî experimenting with semantic search, FAISS/Qdrant vector stores, and structured context injection

- **Automation Integration** ‚Äî embedding LLMs into terminal workflows, scripts, and custom tools to reduce boilerplate, lookup effort, and repetitive coding

I focus on using AI to support **human-led problem solving** ‚Äî not to replace thinking, but to reduce friction, amplify insights, and enable faster, more confident execution.

## ‚öôÔ∏è Developer Tooling

I maintain a lean, fast, and fully version-controlled development environment across Linux and Windows. My setup is built around consistency, automation, and low-friction navigation ‚Äî designed to scale from quick scripting to full data workflows.

- **Neovim (LazyVim)** ‚Äî my primary editor, extended with Lua plugins for LSP, AI assistance, markdown tools, and customized workflows

  - Includes tools like:
    - `mason` ‚Äî LSP installer/manager
    - `conform.nvim` ‚Äî formatter management
    - `nvim-treesitter` ‚Äî syntax-aware parsing and highlighting
    - `nvim-cmp` stack ‚Äî fast, intelligent autocompletion

- **Git & GitHub** ‚Äî used for everything from project and config management to automation and CI/CD

  - I follow a GitHub Flow strategy, extended with personal aliases for feature branches, PRs, and safe merges

- **lazygit** ‚Äî terminal-based Git UI used for staging, reviewing, resolving, and committing changes efficiently

- **ripgrep, fzf, yazi** ‚Äî my go-to stack for fuzzy finding, recursive search, and file navigation in terminal environments

- **Python environment tools** ‚Äî I use [`uv`](https://github.com/astral-sh/uv) for fast dependency management and environment isolation, and [`ruff`](https://github.com/astral-sh/ruff) for linting and formatting

- **Shell automation** ‚Äî portable setup scripts (Bash + PowerShell) to bootstrap Neovim, clone dotfiles, configure Git, and prep new machines

- **GitHub CLI & Actions** ‚Äî used to automate PRs, repo ops, and site/blog deployments in personal and client-facing projects

I treat my tooling like any production system ‚Äî composable, documented, and continuously improved.

## üß™ Scripting & Integration

I rely heavily on scripting to automate data flows, integrate systems, and reduce manual effort across recurring tasks. My scripting environment spans multiple languages and platforms ‚Äî all focused on portability, clarity, and real-world output.

- **Python** ‚Äî my primary language for scripting data pipelines, API connectors, web scraping, and file processing

  - Often used in automation flows to extract data, reshape files, upload results, or trigger downstream tools

- **Shell scripting (Bash & PowerShell)** ‚Äî used for bootstrapping environments, managing config, orchestrating setup flows across Linux and Windows

- **C# scripting (Tabular Editor)** ‚Äî used for automating model changes, managing calculation groups, mass-editing metadata, and generating reusable DAX patterns

- **REST API integration** ‚Äî experience working with JSON, token-based authentication, and platform-specific APIs (e.g., ServiceNow, GitHub, OpenAI)

  - Scripting API calls for data extraction, automation triggers, or status/reporting updates

- **ServiceNow scripting & reporting** ‚Äî building export flows, extracting reporting data, and automating performance analytics exports using the platform's scripting layer

- **Data flow automation** ‚Äî scripting ETL/ELT-like tasks across file systems, Excel, APIs, and databases ‚Äî often bridging structured tools like Power BI with external logic

My scripting focus is always on clarity, reuse, and versionability ‚Äî not throwaway scripts, but modular tools that can scale or adapt as needed.

## üì¶ Data Modeling & Semantics

Designing clean, scalable, and understandable semantic models is central to my work ‚Äî whether for one-off reporting or enterprise-wide datasets. I treat the model layer as a critical interface between raw data and business value.

- **Tabular Modeling** ‚Äî experienced in building and maintaining semantic models using Power BI, Microsoft Fabric, and Tabular Editor

  - Creating reusable measures, hierarchies, calculation groups, and KPI structures
  - Managing relationships, normalization, and performance-optimized schema design
  - Applying role-level security (RLS), perspectives, and documentation standards

- **TMDL (Tabular Model Definition Language)** ‚Äî used for versioning, deploying, and managing semantic models in source control

  - Enabling clean diffing, rollbacks, collaboration, and CI/CD-style deployments
  - Working with TMDL in both Tabular Editor and Microsoft Fabric environments

- **Metadata & Governance** ‚Äî I take a disciplined approach to model metadata

  - Using consistent naming conventions, foldering, and descriptions
  - Supporting documentation via scripting, metadata extraction, and alignment with stakeholders

- **Measure Logic & Performance** ‚Äî I build DAX that balances readability and performance
  - Avoiding unnecessary dependencies or filters
  - Using `CALCULATE`, `REMOVEFILTERS`, `VAR`, and iterator patterns effectively
  - Validating assumptions with tools like DAX Studio and server timings

For me, modeling isn‚Äôt just technical ‚Äî it‚Äôs where context, logic, and usability come together. I build semantic layers that clarify, not complicate.
